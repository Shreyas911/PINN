{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Shreyas911/PINN/blob/main/PINN_Mountain_glacier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2awjNxvTeDt"
   },
   "source": [
    "# Physics Informed Neural Networks for the Mountain glacier model\n",
    "\n",
    "## Install the required libraries\n",
    "\n",
    "We first import all the required libraries. In order to use PyTorch with TPUs, we have to install the TPU client using pip.\n",
    "\n",
    "Note that the pip install command might change based on the version that is currently in use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jiDYi1ksuA-8",
    "outputId": "6e8175fb-1d1c-49c6-9701-2fa829350e43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: torch_xla-1.9-cp37-cp37m-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\n",
      "Looking in links: https://download.pytorch.org/whl/cu111/torch_stable.html\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.9.0+cu111 (from versions: 1.4.0, 1.5.0, 1.5.1, 1.6.0, 1.7.0, 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for torch==1.9.0+cu111\u001b[0m\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/17/k_zh2ys92xvdfdqcy0k40lhw0000gp/T/ipykernel_55910/1143730995.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes_grid1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_axes_locatable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "#!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
    "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
    "!pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchtext==0.10.0 -f https://download.pytorch.org/whl/cu111/torch_stable.html\n",
    "\n",
    "### Import the required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_xla ## Specially for TPUs\n",
    "import torch_xla.core.xla_model as xm ## Specially for TPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "id": "s0WWEOBUUKRz",
    "outputId": "46214fb4-d8bb-432e-9e3a-e073084d4e3e"
   },
   "outputs": [],
   "source": [
    "## Specify the device\n",
    "\n",
    "We now specify the device type. There are two options to leverge the power of parallel computing. We can either use GPUs or TPUs. In this case, we use TPUs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "emsxgANcv-C6",
    "outputId": "31300009-39f8-4852-d3f5-baf204926e92"
   },
   "outputs": [],
   "source": [
    "### GPU ###\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#torch.cuda.empty_cache()\n",
    "###########\n",
    "\n",
    "### TPU ###\n",
    "device = xm.xla_device()\n",
    "###########\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xC8yY_L5UlPi"
   },
   "source": [
    "We now define a class of neural networks which inherits from the *nn.Module* class. It's a 5 layer Artificial Neural Network (ANN). We make use of the tanh activation function. This allows the model to learn highly non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sYrSyy3PWWDf",
    "outputId": "54302b95-7ba6-4932-ce6a-198ec17c5a37"
   },
   "outputs": [],
   "source": [
    "class physics_informed_NN(nn.Module):\n",
    "    \n",
    "    def __init__ (self):\n",
    "\n",
    "        super(physics_informed_NN, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(2, 200)\n",
    "        self.fc2 = nn.Linear(200, 200)\n",
    "        self.fc3 = nn.Linear(200, 200)\n",
    "        self.fc4 = nn.Linear(200, 200)\n",
    "        self.fc5 = nn.Linear(200, 200)\n",
    "        self.fc6 = nn.Linear(200, 200)\n",
    "        self.fc7 = nn.Linear(200, 200)\n",
    "        self.fc8 = nn.Linear(200, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        x = torch.tanh(self.fc4(x))\n",
    "        x = torch.tanh(self.fc5(x))\n",
    "        x = torch.relu(self.fc6(x))\n",
    "        x = torch.tanh(self.fc7(x))\n",
    "        x = self.fc8(x)\n",
    "        return x\n",
    "\n",
    "physics_informed_NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s79RFlqGSYIR"
   },
   "source": [
    "##True solution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EvaB4dD9V2Hj"
   },
   "source": [
    "We now generate the true solution for a simple mountain glacier model, taken from *Fundamentals of Glacier Dynamics* by CJ van der Veen and then look to make our neural network model *emulate* this PDE. The system is modeled by a non-linear, highly diffusive PDE. Although, ice sheet models tend to be non-local, but as a starting point we assume a local model. \n",
    "\n",
    "$$\n",
    " \\frac{\\partial H}{\\partial t } = -\\frac{\\partial}{\\partial x}\\left(-D(x)\\frac{\\partial h}{\\partial x}\\right) + M\\\\\n",
    "  D(x) = CH^{n+2}\\left|\\frac{\\partial h}{\\partial x}\\right|^{n-1}\\\\\n",
    "  C = \\frac{2A}{n+2}(\\rho g)^n\\\\\n",
    "    H(x,t) = h(x,t) - b(x) \\\\ \n",
    "    H_l = 0, H_r > 0\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial{b}}{\\partial{x}} = -0.1\\\\\n",
    "M(x) = M_0 - x M_1 \\:\\text{(accumulation rate, essentially a source term)}\\\\\n",
    "M_0 = 4.0 \\:\\text{m/yr}, \\:M_1 = 0.0002 \\:\\text{yr}^{-1}\\\\\n",
    "\\rho = 920 \\:\\text{kg/m}^3\\\\\n",
    "g = 9.8 \\:\\text{m/s}^2\\\\\n",
    "A = 10^{-16} \\: \\text{Pa}^{-3} \\text{a}^{-1}\\\\\n",
    "n = 3\\\\\n",
    "dx = 1.0 \\:\\text{km}, \\:L = 30 \\:\\text{km}\\\\\n",
    "dt = 1 \\:\\text{month}, \\:T = 2000 \\:\\text{yr}$$\n",
    "\n",
    "The true solution will be generated using a staggered grid finite volume method on a fine, uniform grid.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KiENr9ZuWUe_"
   },
   "outputs": [],
   "source": [
    "def accum_rate_func(x):\n",
    "  M = 0.004-0.0002*x\n",
    "  return M\n",
    "\n",
    "def basal_topology_func(x):\n",
    "  b = 1.0 - 0.01*x\n",
    "  return b\n",
    "\n",
    "def true_solution(L, T, accum_rate_func, basal_topology_func):\n",
    "\n",
    "    dx = 1.0\n",
    "    dt = 1./12.\n",
    "    nx = int(L/dx)\n",
    "    nt = int(T/dt)\n",
    "    x = np.linspace(0,L,nx+1)\n",
    "    t = np.linspace(0,T,nt+1)\n",
    "\n",
    "    M = accum_rate_func(x)\n",
    "    b = basal_topology_func(x)\n",
    "\n",
    "    A = 1e-16\n",
    "    rho = 920.0\n",
    "    g = 9.2 \n",
    "    n = 3\n",
    "\n",
    "    C = 2*A/(n+2) * (rho*g)**n * (1e3)**n\n",
    "\n",
    "    print(f\"True value of C = {C}\")\n",
    "\n",
    "    h = np.zeros((nx+1,nt+1))\n",
    "    H = np.zeros((nx+1,nt+1))\n",
    "    h[:,0] = b\n",
    "    h[0,:] = b[0]\n",
    "    h[-1,:] = b[-1]\n",
    "\n",
    "    H[:,0] = h[:,0] - b\n",
    "    H[0,:] = h[0,:] - b[0]\n",
    "    H[-1,:] = h[-1,:] - b[-1]\n",
    "\n",
    "    for i in range(1,len(t)):\n",
    "\n",
    "        D = C *((H[1:,i-1]+H[:nx,i-1])/2.0)**(n+2) * ((h[1:,i-1] - h[:nx,i-1])/dx)**(n-1)\n",
    "\n",
    "        phi = -D*(h[1:,i-1]-h[:nx,i-1])/dx\n",
    "\n",
    "        h[1:nx,i] = h[1:nx,i-1] + M[1:nx]*dt - dt/dx * (phi[1:]-phi[:nx-1])\n",
    "        h[1:nx,i] = (h[1:nx,i] < b[1:nx]) * b[1:nx] + (h[1:nx,i] >= b[1:nx]) * h[1:nx,i]\n",
    "        H[:,i] = h[:,i] - b\n",
    "\n",
    "    return x, t, H, h, C, n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ik2w_4aTPHcu"
   },
   "source": [
    "Here is the contourf plot for the true solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0bbHTSn9O0B",
    "outputId": "74d744b5-c44e-48ea-df42-88a158b81039"
   },
   "outputs": [],
   "source": [
    "L = 30.\n",
    "T = 2000.\n",
    "\n",
    "x, t, H_true, h_true, C_true, n_true = true_solution(L, T, accum_rate_func, basal_topology_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "I-DuH5plLoXT",
    "outputId": "5819a825-4b8f-4df4-c504-81c8d37850b7"
   },
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "\n",
    "print(x.shape, t.shape)\n",
    "norm = mcolors.TwoSlopeNorm(vmin=-1, vmax = h_true.max(), vcenter=0)\n",
    "plt.contourf(np.reshape(t, (-1)),np.reshape(x, (-1)),h_true, cmap = 'jet', levels = 50)\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('x')\n",
    "plt.title('True Solution for h')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bI0n1rMSVuW"
   },
   "source": [
    "## How do we actually train our network?\n",
    "\n",
    "Since we are training our network to emulate the PDE, we want to penalize the physics not being followed. We define the Loss function as follows (superscripts indicate the data) - \n",
    "\n",
    "\n",
    "$$\\text{PDE operator} \\quad \\mathcal{F}(x, t) = H_t (x,t) - 3 (C H^5 h_x^3)_x  - M(x)$$\n",
    "$$\\text{Loss function} \\quad \\mathcal{L} = \\frac{1}{N_i}\\sum_{i=1}^{N_i} \\left(h^i - h(x^i,0)\\right)^2 + \\frac{1}{N_b}\\sum_{i=1}^{N_b} \\left(h^i - h(x^i, t^i)\\right)^2 + \\frac{1}{N_c}\\sum_{i=1}^{N_c} \\left(\\mathcal{F}(x^i, t^i) \\right)^2 + \\frac{1}{N_d}\\sum_{i=1}^{N_d} \\left(h^i - h(x^i, t^i)\\right)^2$$\n",
    "\n",
    "where, $N_i$, $N_b$, $N_c$ are the initial value points, boundary value points and collocation points in the interior where we want the model to obey the physics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFfgPCnJfxux"
   },
   "source": [
    "We only need to randomly sample the domain and boundaries to feed these points to the neural network. We will use $N_i = 50, N_b = 100, N_c = 20,000$ points to train the neural network, which is pretty sparse. The snippet below is to prepare the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0ANuzj-NRpK"
   },
   "outputs": [],
   "source": [
    "x = x.reshape(-1,1)\n",
    "t = t.reshape(-1,1)\n",
    "\n",
    "### Initial data\n",
    "Ni = 25\n",
    "random_i = np.random.permutation(x.shape[0])[:Ni]\n",
    "x_i = x[random_i]\n",
    "t_i = np.zeros((Ni, 1))\n",
    "h_i = np.reshape(h_true[random_i, 0], (-1,1))\n",
    "\n",
    "### Boundary data\n",
    "Nb = 1000\n",
    "Nb = int(Nb/2) # Split points to two boundaries\n",
    "\n",
    "random_b = np.random.permutation(t.shape[0])[:Nb]\n",
    "x_l = np.zeros((Nb, 1))\n",
    "x_r = L*np.ones((Nb, 1))\n",
    "t_b = t[random_b]\n",
    "h_l = np.reshape(h_true[0, random_b], (-1,1))\n",
    "h_r = np.reshape(h_true[-1, random_b], (-1,1))\n",
    "\n",
    "### Collocation points - sample such that you don't sample the boundary and initial points again\n",
    "Nc = 140000\n",
    "\n",
    "x_grid = np.tile(x,(1, t.shape[0]))\n",
    "t_grid = np.tile(t.T,(x.shape[0], 1))\n",
    "\n",
    "temp = np.zeros((x.shape[0]-2)*(t.shape[0]-1), dtype = bool)\n",
    "temp[:Nc] = 1\n",
    "temp = np.random.permutation(temp)\n",
    "temp = np.reshape(temp, (x.shape[0]-2, t.shape[0]-1))\n",
    "random_c = np.zeros((x.shape[0], t.shape[0]), dtype = bool)\n",
    "random_c[1:-1,1:] = temp\n",
    "x_c = x_grid[random_c].reshape(-1,1)\n",
    "t_c = t_grid[random_c].reshape(-1,1)\n",
    "h_c = h_true[random_c].reshape(-1,1)\n",
    "\n",
    "### Data points - sample such that you don't sample the boundary and initial points again\n",
    "Nd = 10000\n",
    "\n",
    "temp = np.zeros((x.shape[0]-2)*(t.shape[0]-1), dtype = bool)\n",
    "temp[:Nd] = 1\n",
    "temp = np.random.permutation(temp)\n",
    "temp = np.reshape(temp, (x.shape[0]-2, t.shape[0]-1))\n",
    "random_d = np.zeros((x.shape[0], t.shape[0]), dtype = bool)\n",
    "random_d[1:-1,1:] = temp\n",
    "x_d = x_grid[random_d].reshape(-1,1)\n",
    "t_d = t_grid[random_d].reshape(-1,1)\n",
    "h_d = h_true[random_d].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFmHbRN-yv1c"
   },
   "source": [
    "We plot the points we have sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "vw3VK3OnywOl",
    "outputId": "e2b3f6d2-3bd5-4d27-f662-23b3de287fc3"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,15))\n",
    "plt.gca().set_aspect(10)\n",
    "plt.scatter(t_i, x_i, label = \"Initial\")\n",
    "plt.scatter(t_b, x_l, label = \"Boundary\")\n",
    "plt.scatter(t_b, x_r, label = \"Boundary\")\n",
    "plt.scatter(t_c, x_c, label = \"Collocation\", s = 0.2)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klx7mISbv8L6"
   },
   "source": [
    "We have our data and we now convert it to torch tensors and concatenate the respective $x,t$ tensors to feed to the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGrrYBpnw55F"
   },
   "outputs": [],
   "source": [
    "x_torch_i = torch.from_numpy(x_i).float()\n",
    "t_torch_i = torch.from_numpy(t_i).float()\n",
    "h_torch_i = torch.from_numpy(h_i).float()\n",
    "\n",
    "x_torch_l = torch.from_numpy(x_l).float()\n",
    "x_torch_r = torch.from_numpy(x_r).float()\n",
    "t_torch_b = torch.from_numpy(t_b).float()\n",
    "h_torch_l = torch.from_numpy(h_l).float()\n",
    "h_torch_r = torch.from_numpy(h_r).float()\n",
    "\n",
    "x_torch_c = torch.from_numpy(x_c).float()\n",
    "t_torch_c = torch.from_numpy(t_c).float()\n",
    "h_torch_c = torch.from_numpy(h_c).float()\n",
    "\n",
    "x_torch_d = torch.from_numpy(x_d).float()\n",
    "t_torch_d = torch.from_numpy(t_d).float()\n",
    "h_torch_d = torch.from_numpy(h_d).float()\n",
    "\n",
    "### Concatenate the respective x and t tensors \n",
    "X_i = torch.cat((x_torch_i, t_torch_i), 1)\n",
    "\n",
    "X_l = torch.cat((x_torch_l, t_torch_b), 1)\n",
    "X_r = torch.cat((x_torch_r, t_torch_b), 1)\n",
    "\n",
    "X_c = torch.cat((x_torch_c, t_torch_c), 1)\n",
    "\n",
    "X_d = torch.cat((x_torch_d, t_torch_d), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWoI00yL1I-i"
   },
   "source": [
    "Now we transfer this data, as well as an object of the *physics_informed_NN* class to the TPUs to run our computatations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PoThOJMzxNp-"
   },
   "outputs": [],
   "source": [
    "PINN = physics_informed_NN()\n",
    "PINN.to(device)\n",
    "\n",
    "X_i = X_i.to(device)\n",
    "H_small_i = h_torch_i.to(device)\n",
    "\n",
    "X_l = X_l.to(device)\n",
    "H_small_l = h_torch_l.to(device)\n",
    "X_r = X_r.to(device)\n",
    "H_small_r = h_torch_r.to(device)\n",
    "\n",
    "X_c = X_c.to(device)\n",
    "H_small_c = h_torch_c.to(device)\n",
    "\n",
    "X_d = X_d.to(device)\n",
    "H_small_d = h_torch_d.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "akAMcnrS12Os"
   },
   "source": [
    "Using the loss function defined above, we run backpropagation on the neural network using the *Adam* optimizer. Our criterion is obviously MSE Loss and we also have a scheduler to control the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVhg47N_6iff"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(PINN.parameters())\n",
    "criterion = nn.MSELoss()\n",
    "my_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=200, gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXFmm3Rx_pqK"
   },
   "source": [
    "The loop below is actually training the network. The epochs just mean the number of times the whole dataset is exposed to the neural network. Note that running on TPUs requires slightly different commands for optimizer steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s-Qek4Xq1xuG",
    "outputId": "364f9297-b9b2-4baa-b60a-b49b753334a4"
   },
   "outputs": [],
   "source": [
    "statistics = []\n",
    "for epochs in range(10000):\n",
    "\n",
    "    X_c_clone = X_c.clone()\n",
    "    X_c_clone.requires_grad = True\n",
    "    h_pred = PINN(X_c_clone)\n",
    "\n",
    "    b = 1.0 - 0.01*X_c_clone[:,0]\n",
    "    b = torch.reshape(b, (-1,1))\n",
    "    M = 0.004-0.0002*X_c_clone[:,0]\n",
    "\n",
    "    H_pred = torch.sub(h_pred, b)\n",
    "    gradients_h = torch.autograd.grad(torch.sum(h_pred), X_c_clone, create_graph=True)\n",
    "    h_x = gradients_h[0][:,0]\n",
    "    h_t = gradients_h[0][:,1]\n",
    "    hessians_h = torch.autograd.grad(torch.sum(h_x), X_c_clone, create_graph=True)\n",
    "    h_xx = hessians_h[0][:,0]\n",
    "\n",
    "    gradients_H = torch.autograd.grad(torch.sum(H_pred), X_c_clone, create_graph=True)\n",
    "    H_x = gradients_H[0][:,0]\n",
    "\n",
    "    flux = - C_true * torch.pow(torch.squeeze(H_pred), 5) * torch.pow(torch.abs(h_x), 2) * h_x\n",
    "    gradients_flux = torch.autograd.grad(torch.sum(flux), X_c_clone, create_graph=True)\n",
    "    flux_x = - gradients_flux[0][:,0]\n",
    "    pde_rhs = M + flux_x\n",
    "    collocation_mse = criterion(h_t, pde_rhs)\n",
    "    initial_mse = criterion(PINN(X_i), H_small_i)\n",
    "    left_mse = criterion(PINN(X_l), H_small_l)\n",
    "    right_mse = criterion(PINN(X_r), H_small_r)\n",
    "    data_mse = criterion(PINN(X_d), H_small_d)\n",
    "\n",
    "    loss = right_mse + left_mse + initial_mse + 100. * collocation_mse + 10. * data_mse\n",
    "    optimizer.zero_grad()  \n",
    "    loss.backward()     \n",
    "    xm.optimizer_step(optimizer)\n",
    "    xm.mark_step()\n",
    "    my_lr_scheduler.step()\n",
    "\n",
    "    if (epochs % 100 == 0):\n",
    "        statistics.append([epochs, loss.cpu().data.numpy(), left_mse.cpu().data.numpy(), right_mse.cpu().data.numpy(), initial_mse.cpu().data.numpy(), collocation_mse.cpu().data.numpy(), data_mse.cpu().data.numpy()])\n",
    "        print (f'epoch = {epochs}, loss = {loss}, left_loss = {left_mse}, right_loss = {right_mse}, initial_mse = {initial_mse}, collocation_mse = {collocation_mse}, data_mse = {data_mse}')\n",
    "        # print (f'epoch = {epochs}, C = {C_true}, loss = {loss}, left_loss = {left_mse}, right_loss = {right_mse}, initial_mse = {initial_mse}, collocation_mse = {collocation_mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOnVqq8RByUR"
   },
   "source": [
    "We now check the performance of our model on the entire grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1d02R8dP6dlT"
   },
   "outputs": [],
   "source": [
    "## Grid x,t together for test dataa\n",
    "test_data = np.array(np.meshgrid(x, t))\n",
    "test_data = np.moveaxis(test_data, [0, 1, 2], [-1,0,1])\n",
    "input_test_data = torch.from_numpy(np.reshape(test_data, (-1, 2)))\n",
    "input_test_data = input_test_data.to(device).float()\n",
    "\n",
    "### Actual solution on the entire grid\n",
    "h_test_data = torch.from_numpy(np.reshape(h_true, (-1,1)))\n",
    "\n",
    "### Predict on the entire grid\n",
    "with torch.no_grad():\n",
    "##### FOR TPUs ######\n",
    "    h_predicted = PINN(input_test_data).cpu().data.numpy()\n",
    "\n",
    "##### FOR CUDA/GPUs ######\n",
    "#   if torch.cuda.is_available():\n",
    "#     h_predicted = PINN(input_test_data).cpu().data.numpy()\n",
    "#   else:\n",
    "#     h_predicted = PINN(input_test_data).data.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hF0AzRmD0Lh"
   },
   "source": [
    "Now, we plot our predicted solution. Since *u_predicted* is a single vector, we need to reshaoe it into the shape of the grid in order to plot the filled contours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "id": "BcOVSgt29HmD",
    "outputId": "746c900c-4b14-498c-c278-b4942d8fe8f9"
   },
   "outputs": [],
   "source": [
    "norm = mcolors.TwoSlopeNorm(vmin=-1, vmax = h_predicted.max(), vcenter=0)\n",
    "h_pred_final = np.reshape(h_predicted, (h_true.shape[1], h_true.shape[0])).T\n",
    "\n",
    "plt.contourf(np.squeeze(t),np.squeeze(x),h_pred_final, cmap = 'jet', levels = 50)\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('x')\n",
    "plt.title('Emulated Solution for h')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "id": "Ms5Pu5FjjhT_",
    "outputId": "b07338ce-3c07-470e-a436-7ae41b9093c2"
   },
   "outputs": [],
   "source": [
    "plt.contourf(np.squeeze(t),np.squeeze(x),(h_pred_final-h_true), cmap = 'jet', levels = 50)\n",
    "plt.colorbar()\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('x')\n",
    "plt.title('Emulated - Predicted solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "pVgNPYuVNJYX",
    "outputId": "963a4c3d-65cd-4d46-852e-23d63300b883"
   },
   "outputs": [],
   "source": [
    "# plt.plot(h_pred_final)\n",
    "plt.plot(h_pred_final[25,:], label = 'Emulated')\n",
    "plt.plot(h_true[25,:], label = 'True')\n",
    "plt.title('Profile for a given x')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "fXouI__jNJYZ",
    "outputId": "fec533fb-40bb-4963-9f96-9b96c786dca3"
   },
   "outputs": [],
   "source": [
    "plt.plot(h_pred_final[:,1500], label = 'Emulated')\n",
    "plt.plot(h_true[:,1500], label = 'True')\n",
    "plt.title('Profile for a given t')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "1yrvMvZcNJYZ",
    "outputId": "4fc824a1-0d15-4fa7-f715-1b946f2b2737"
   },
   "outputs": [],
   "source": [
    "plt.plot(h_pred_final[0,:], label = 'Emulated')\n",
    "plt.plot(h_true[0,:], label = 'True')\n",
    "plt.title('Left boundary condition')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "AsNa8sivNJYa",
    "outputId": "38415842-e97f-48d6-dabe-45d1103d1878"
   },
   "outputs": [],
   "source": [
    "plt.plot(h_pred_final[-1,:], label = 'Emulated')\n",
    "plt.plot(h_true[-1,:], label = 'True')\n",
    "plt.title('Right boundary condition')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "qpdBqagcNJYa",
    "outputId": "4272d8c5-4d86-437f-aa4a-570eb0d90fe6"
   },
   "outputs": [],
   "source": [
    "plt.plot(h_pred_final[:,0], label = 'Emulated')\n",
    "plt.plot(h_true[:,0], label = 'True')\n",
    "plt.title('Initial condition')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "R3UaDgsCW80Y",
    "outputId": "4d61bab6-f455-43fd-8ca2-747b0aa7111e"
   },
   "outputs": [],
   "source": [
    "# plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot([statistic[0] for statistic in statistics], [statistic[1] for statistic in statistics], label = 'Total loss')\n",
    "plt.plot([statistic[0] for statistic in statistics], [statistic[2] for statistic in statistics], label = 'Left loss')\n",
    "plt.plot([statistic[0] for statistic in statistics], [statistic[3] for statistic in statistics], label = 'Right loss')\n",
    "plt.plot([statistic[0] for statistic in statistics], [statistic[4] for statistic in statistics], label = 'Initial loss')\n",
    "plt.plot([statistic[0] for statistic in statistics], [statistic[5] for statistic in statistics], label = 'Collocation loss')\n",
    "plt.plot([statistic[0] for statistic in statistics], [statistic[6] for statistic in statistics], label = 'Data loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6X8fyNFj7dg"
   },
   "source": [
    "## Inferring accumulation rate from data\n",
    "\n",
    "Since we are training our network to find the diffusion coefficient, we want to enforce the physics, as well look to fit the data. We define the Loss function as follows (superscripts indicate the data) - \n",
    "\n",
    "\n",
    "$$\\text{PDE operator} \\quad \\mathcal{F}(x, t) = H_t (x,t) - 3 (C H^5 h_x^3)_x  - M(x) $$\n",
    "$$\\text{Loss function} \\quad \\mathcal{L} = \\frac{1}{N_i}\\sum_{i=1}^{N_i} \\left(h^i - h(x^i,0)\\right)^2 + \\frac{1}{N_b}\\sum_{i=1}^{N_b} \\left(h^i - h(x^i, t^i)\\right)^2 + \\frac{1}{N_c}\\sum_{i=1}^{N_c} \\left(\\mathcal{F}(x^i, t^i) \\right)^2 + \\frac{1}{N_d}\\sum_{i=1}^{N_d} \\left(h^i - h(x^i, t^i)\\right)^2$$\n",
    "\n",
    "where, $N_i$, $N_b$, $N_c$ are the initial value points, boundary value points and collocation points in the interior where we want the model to obey the physics.\n",
    "\n",
    "Here, we optimize wrt both the neural network parameters as well as the unknown accumulation rate $M$. $M$ is modeled as $M=B+Ax$ and we look to infer the coeffients A and B. $B_{\\text{true}} = 0.004, A_{\\text{true}} = -0.0002$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hOQeqrAj3pry",
    "outputId": "546e65bf-e814-4d41-ff3a-f9868ee2ab25"
   },
   "outputs": [],
   "source": [
    "class physics_informed_NN(nn.Module):\n",
    "    \n",
    "    def __init__ (self):\n",
    "\n",
    "        super(physics_informed_NN, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(2, 200)\n",
    "        self.fc2 = nn.Linear(200, 200)\n",
    "        self.fc3 = nn.Linear(200, 200)\n",
    "        self.fc4 = nn.Linear(200, 200)\n",
    "        self.fc5 = nn.Linear(200, 200)\n",
    "        self.fc6 = nn.Linear(200, 200)\n",
    "        self.fc7 = nn.Linear(200, 200)\n",
    "        self.fc8 = nn.Linear(200, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        x = torch.tanh(self.fc4(x))\n",
    "        x = torch.tanh(self.fc5(x))\n",
    "        x = torch.tanh(self.fc6(x))\n",
    "        x = torch.tanh(self.fc7(x))\n",
    "        x = self.fc8(x)\n",
    "        return x\n",
    "\n",
    "physics_informed_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Kh8vy1arVa8"
   },
   "outputs": [],
   "source": [
    "x = x.reshape(-1,1)\n",
    "t = t.reshape(-1,1)\n",
    "\n",
    "### Initial data\n",
    "Ni = 25\n",
    "random_i = np.random.permutation(x.shape[0])[:Ni]\n",
    "x_i = x[random_i]\n",
    "t_i = np.zeros((Ni, 1))\n",
    "h_i = np.reshape(h_true[random_i, 0], (-1,1))\n",
    "\n",
    "### Boundary data\n",
    "Nb = 1000\n",
    "Nb = int(Nb/2) # Split points to two boundaries\n",
    "\n",
    "random_b = np.random.permutation(t.shape[0])[:Nb]\n",
    "x_l = np.zeros((Nb, 1))\n",
    "x_r = L*np.ones((Nb, 1))\n",
    "t_b = t[random_b]\n",
    "h_l = np.reshape(h_true[0, random_b], (-1,1))\n",
    "h_r = np.reshape(h_true[-1, random_b], (-1,1))\n",
    "\n",
    "### Collocation points - sample such that you don't sample the boundary and initial points again\n",
    "Nc = 75000\n",
    "\n",
    "x_grid = np.tile(x,(1, t.shape[0]))\n",
    "t_grid = np.tile(t.T,(x.shape[0], 1))\n",
    "\n",
    "temp = np.zeros((x.shape[0]-2)*(t.shape[0]-1), dtype = bool)\n",
    "temp[:Nc] = 1\n",
    "temp = np.random.permutation(temp)\n",
    "temp = np.reshape(temp, (x.shape[0]-2, t.shape[0]-1))\n",
    "random_c = np.zeros((x.shape[0], t.shape[0]), dtype = bool)\n",
    "random_c[1:-1,1:] = temp\n",
    "x_c = x_grid[random_c].reshape(-1,1)\n",
    "t_c = t_grid[random_c].reshape(-1,1)\n",
    "h_c = h_true[random_c].reshape(-1,1)\n",
    "\n",
    "### Data points - sample such that you don't sample the boundary and initial points again\n",
    "Nd = 75000\n",
    "\n",
    "temp = np.zeros((x.shape[0]-2)*(t.shape[0]-1), dtype = bool)\n",
    "temp[:Nd] = 1\n",
    "temp = np.random.permutation(temp)\n",
    "temp = np.reshape(temp, (x.shape[0]-2, t.shape[0]-1))\n",
    "random_d = np.zeros((x.shape[0], t.shape[0]), dtype = bool)\n",
    "random_d[1:-1,1:] = temp\n",
    "x_d = x_grid[random_d].reshape(-1,1)\n",
    "t_d = t_grid[random_d].reshape(-1,1)\n",
    "h_d = h_true[random_d].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5yBhALKrVbN"
   },
   "source": [
    "We plot the points we have sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "NyqPsqTKrVbN",
    "outputId": "a7bfe320-b43b-4704-8ebb-2d756b4cdbae"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,15))\n",
    "plt.gca().set_aspect(10)\n",
    "plt.scatter(t_i, x_i, label = \"Initial\")\n",
    "plt.scatter(t_b, x_l, label = \"Boundary\")\n",
    "plt.scatter(t_b, x_r, label = \"Boundary\")\n",
    "plt.scatter(t_c, x_c, label = \"Collocation\", s = 0.2)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIgc78gmrVbO"
   },
   "source": [
    "We have our data and we now convert it to torch tensors and concatenate the respective $x,t$ tensors to feed to the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adx3ZVZ5rVbP"
   },
   "outputs": [],
   "source": [
    "x_torch_i = torch.from_numpy(x_i).float()\n",
    "t_torch_i = torch.from_numpy(t_i).float()\n",
    "h_torch_i = torch.from_numpy(h_i).float()\n",
    "\n",
    "x_torch_l = torch.from_numpy(x_l).float()\n",
    "x_torch_r = torch.from_numpy(x_r).float()\n",
    "t_torch_b = torch.from_numpy(t_b).float()\n",
    "h_torch_l = torch.from_numpy(h_l).float()\n",
    "h_torch_r = torch.from_numpy(h_r).float()\n",
    "\n",
    "x_torch_c = torch.from_numpy(x_c).float()\n",
    "t_torch_c = torch.from_numpy(t_c).float()\n",
    "h_torch_c = torch.from_numpy(h_c).float()\n",
    "\n",
    "x_torch_d = torch.from_numpy(x_d).float()\n",
    "t_torch_d = torch.from_numpy(t_d).float()\n",
    "h_torch_d = torch.from_numpy(h_d).float()\n",
    "\n",
    "### Concatenate the respective x and t tensors \n",
    "X_i = torch.cat((x_torch_i, t_torch_i), 1)\n",
    "\n",
    "X_l = torch.cat((x_torch_l, t_torch_b), 1)\n",
    "X_r = torch.cat((x_torch_r, t_torch_b), 1)\n",
    "\n",
    "X_c = torch.cat((x_torch_c, t_torch_c), 1)\n",
    "\n",
    "X_d = torch.cat((x_torch_d, t_torch_d), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EbzzbA-rVbP"
   },
   "source": [
    "Now we transfer this data, as well as an object of the *physics_informed_NN* class to the TPUs to run our computatations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P2Qxiki7rVbP"
   },
   "outputs": [],
   "source": [
    "PINN = physics_informed_NN()\n",
    "PINN.to(device)\n",
    "\n",
    "X_i = X_i.to(device)\n",
    "H_small_i = h_torch_i.to(device)\n",
    "\n",
    "X_l = X_l.to(device)\n",
    "H_small_l = h_torch_l.to(device)\n",
    "X_r = X_r.to(device)\n",
    "H_small_r = h_torch_r.to(device)\n",
    "\n",
    "X_c = X_c.to(device)\n",
    "H_small_c = h_torch_c.to(device)\n",
    "\n",
    "X_d = X_d.to(device)\n",
    "H_small_d = h_torch_d.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1AqIkTvprVbP"
   },
   "source": [
    "Using the loss function defined above, we run backpropagation on the neural network using the *Adam* optimizer. Our criterion is obviously MSE Loss and we also have a scheduler to control the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uNE1DftQrVbP"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(PINN.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "my_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=200, gamma=0.9)\n",
    "# my_lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer)\n",
    "#C = torch.tensor(0., requires_grad = True)\n",
    "\n",
    "# M = torch.zeros((Nc), requires_grad = True, dtype = float, device = device)\n",
    "A = torch.tensor(0., requires_grad = True, device = device)\n",
    "B = torch.tensor(0., requires_grad = True, device = device)\n",
    "optimizer.add_param_group({'params': A})\n",
    "optimizer.add_param_group({'params': B})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ql0Bc8e1rVbQ"
   },
   "source": [
    "The loop below is actually training the network. The epochs just mean the number of times the whole dataset is exposed to the neural network. Note that running on TPUs requires slightly different commands for optimizer steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2XMd0paNrVbQ",
    "outputId": "6978fe66-53a3-4696-fa7d-8cd6422c48a2"
   },
   "outputs": [],
   "source": [
    "statistics = []\n",
    "for epochs in range(15000):\n",
    "\n",
    "    X_c_clone = X_c.clone()\n",
    "    X_c_clone.requires_grad = True\n",
    "    h_pred = PINN(X_c_clone)\n",
    "\n",
    "    b = 1.0 - 0.01*X_c_clone[:,0]\n",
    "    b = torch.reshape(b, (-1,1))\n",
    "    # M = 0.004-0.0002*X_c_clone[:,0]\n",
    "\n",
    "    H_pred = torch.sub(h_pred, b)\n",
    "    gradients_h = torch.autograd.grad(torch.sum(h_pred), X_c_clone, create_graph=True)\n",
    "    h_x = gradients_h[0][:,0]\n",
    "    h_t = gradients_h[0][:,1]\n",
    "    hessians_h = torch.autograd.grad(torch.sum(h_x), X_c_clone, create_graph=True)\n",
    "    h_xx = hessians_h[0][:,0]\n",
    "\n",
    "    gradients_H = torch.autograd.grad(torch.sum(H_pred), X_c_clone, create_graph=True)\n",
    "    H_x = gradients_H[0][:,0]\n",
    "\n",
    "    flux = - C_true * torch.pow(torch.squeeze(H_pred), 5) * torch.pow(torch.abs(h_x), 2) * h_x\n",
    "    gradients_flux = torch.autograd.grad(torch.sum(flux), X_c_clone, create_graph=True)\n",
    "    flux_x = - gradients_flux[0][:,0]\n",
    "    M = B + A*X_c_clone[:, 0]\n",
    "    pde_rhs = M + flux_x\n",
    "    collocation_mse = criterion(h_t, pde_rhs)\n",
    "    initial_mse = criterion(PINN(X_i), H_small_i)\n",
    "    left_mse = criterion(PINN(X_l), H_small_l)\n",
    "    right_mse = criterion(PINN(X_r), H_small_r)\n",
    "    data_mse = criterion(PINN(X_d), H_small_d)\n",
    "\n",
    "    loss = right_mse + left_mse + initial_mse + 75*collocation_mse + 25*data_mse\n",
    "\n",
    "    optimizer.zero_grad()  \n",
    "    loss.backward()       \n",
    "    xm.optimizer_step(optimizer)\n",
    "    xm.mark_step()\n",
    "    my_lr_scheduler.step()\n",
    "\n",
    "    if (epochs % 100 == 0):\n",
    "        statistics.append([epochs, A.detach().clone().cpu().data.numpy(), B.detach().clone().cpu().data.numpy(), loss.cpu().data.numpy(), left_mse.cpu().data.numpy(), right_mse.cpu().data.numpy(), initial_mse.cpu().data.numpy(), collocation_mse.cpu().data.numpy(), data_mse.cpu().data.numpy()])\n",
    "        print (f'epoch = {epochs}, lr = {my_lr_scheduler.get_last_lr()},  loss = {loss}, A = {A}, B = {B}, l_loss = {left_mse}, r_loss = {right_mse}, i_mse = {initial_mse}, c_mse = {collocation_mse}, d_mse = {data_mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NrHxhEun16SA"
   },
   "source": [
    "We now check the performance of our model on the entire grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WEeZoZPR16SB"
   },
   "outputs": [],
   "source": [
    "## Grid x,t together for test dataa\n",
    "test_data = np.array(np.meshgrid(x, t))\n",
    "test_data = np.moveaxis(test_data, [0, 1, 2], [-1,0,1])\n",
    "input_test_data = torch.from_numpy(np.reshape(test_data, (-1, 2)))\n",
    "input_test_data = input_test_data.to(device).float()\n",
    "\n",
    "### Actual solution on the entire grid\n",
    "h_test_data = torch.from_numpy(np.reshape(h_true, (-1,1)))\n",
    "\n",
    "### Predict on the entire grid\n",
    "with torch.no_grad():\n",
    "##### FOR TPUs ######\n",
    "    h_predicted = PINN(input_test_data).cpu().data.numpy()\n",
    "\n",
    "##### FOR CUDA/GPUs ######\n",
    "#   if torch.cuda.is_available():\n",
    "#     h_predicted = PINN(input_test_data).cpu().data.numpy()\n",
    "#   else:\n",
    "#     h_predicted = PINN(input_test_data).data.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6SgXfD316SB"
   },
   "source": [
    "Now, we plot our predicted solution. Since *u_predicted* is a single vector, we need to reshaoe it into the shape of the grid in order to plot the filled contours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "id": "BBw8RKYL16SC",
    "outputId": "5f2f5c34-bf21-4dd5-eb4d-4bd28b0f946b"
   },
   "outputs": [],
   "source": [
    "# norm = mcolors.TwoSlopeNorm(vmin=h_predicted.min(), vmax = h_predicted.max(), vcenter=0)\n",
    "h_pred_final = np.reshape(h_predicted, (h_true.shape[1], h_true.shape[0])).T\n",
    "\n",
    "plt.contourf(np.squeeze(t),np.squeeze(x),h_pred_final, cmap = 'jet', levels = 50)\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('x')\n",
    "plt.title('Emulated Solution for h')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FnIQJy0I5GUx",
    "outputId": "fb0f78bc-a38f-4dcc-ca09-193c0bf85b39"
   },
   "outputs": [],
   "source": [
    "A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "EsTuKC4iaU9K",
    "outputId": "5e3187ef-5f3d-4c61-bd40-679d0d29d026"
   },
   "outputs": [],
   "source": [
    "plt.plot([statistic[0] for statistic in statistics], [statistic[1] for statistic in statistics], label = 'Predicted A')\n",
    "\n",
    "plt.axhline(-0.0002, label = 'Actual A', color = 'red')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "QgFkD4TGasgf",
    "outputId": "f008e73b-e688-4fdb-aeb9-2b388d78e4af"
   },
   "outputs": [],
   "source": [
    "plt.plot([statistic[0] for statistic in statistics], [statistic[2] for statistic in statistics], label = 'Predicted B')\n",
    "\n",
    "plt.axhline(0.004, label = 'Actual B', color = 'red')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "jAOMBYRIc7Kx",
    "outputId": "76e2b547-87d1-4b81-f7d3-75ac63a12f12"
   },
   "outputs": [],
   "source": [
    "# plt.plot(h_pred_final)\n",
    "plt.plot(h_pred_final[25,:], label = 'Emulated')\n",
    "plt.plot(h_true[25,:], label = 'True')\n",
    "plt.title('Profile for a given x')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "wOBM3MF4AVax",
    "outputId": "fdea0641-511e-4219-a9ca-403398d7e51d"
   },
   "outputs": [],
   "source": [
    "plt.plot(h_pred_final[:,1500], label = 'Emulated')\n",
    "plt.plot(h_true[:,1500], label = 'True')\n",
    "plt.title('Profile for a given t')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "vEal4EXw3g4q",
    "outputId": "2f250bd6-93fa-4bad-b5da-0137c7752549"
   },
   "outputs": [],
   "source": [
    "plt.plot(h_pred_final[0,:], label = 'Emulated')\n",
    "plt.plot(h_true[0,:], label = 'True')\n",
    "plt.title('Left boundary condition')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "fS7l0_vTMw62",
    "outputId": "8197c2bf-c8e0-4390-c5e1-960db6aa2811"
   },
   "outputs": [],
   "source": [
    "plt.plot(h_pred_final[-1,:], label = 'Emulated')\n",
    "plt.plot(h_true[-1,:], label = 'True')\n",
    "plt.title('Right boundary condition')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "ywpDDQ-CL3RW",
    "outputId": "b47abbd0-e0e6-43c3-b697-4f317380a5c8"
   },
   "outputs": [],
   "source": [
    "plt.plot(h_pred_final[:,0], label = 'Emulated')\n",
    "plt.plot(h_true[:,0], label = 'True')\n",
    "plt.title('Initial condition')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "hW6KMTVKXsJl",
    "outputId": "3f78b574-886c-4edd-d20a-859bd446fe4b"
   },
   "outputs": [],
   "source": [
    "# plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot([statistic[0] for statistic in statistics], [statistic[3] for statistic in statistics], label = 'Total loss')\n",
    "plt.plot([statistic[0] for statistic in statistics], [statistic[4] for statistic in statistics], label = 'Left loss')\n",
    "plt.plot([statistic[0] for statistic in statistics], [statistic[5] for statistic in statistics], label = 'Right loss')\n",
    "plt.plot([statistic[0] for statistic in statistics], [statistic[6] for statistic in statistics], label = 'Initial loss')\n",
    "plt.plot([statistic[0] for statistic in statistics], [statistic[7] for statistic in statistics], label = 'Collocation loss')\n",
    "plt.plot([statistic[0] for statistic in statistics], [statistic[8] for statistic in statistics], label = 'Data loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ykzJimNuueH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "s0WWEOBUUKRz",
    "s79RFlqGSYIR",
    "0bI0n1rMSVuW"
   ],
   "include_colab_link": true,
   "name": "PINN_Mountain_glacier.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
